include "X64.Vale.InsBasic.vaf"
include "X64.Vale.InsMem.vaf"
include{:fstar}{:open} "Words_s"
include{:fstar}{:open} "Words.Seq_s"
include{:fstar}{:open} "Words.Two_s"
include{:fstar}{:open} "Words.Four_s"
include{:fstar}{:open} "Types_s"
include{:fstar}{:open} "Arch.Types"
include{:fstar}{:open} "X64.Machine_s"
include{:fstar}{:open} "X64.Vale.State"
include{:fstar}{:open} "X64.Vale.Decls"
include{:fstar}{:open} "X64.Vale.QuickCode"
include{:/*TODO*/fstar}{:open Seq} "FStar.Seq.Base"

module X64.Vale.InsVector

#verbatim{:interface}
open Words_s
open Words.Seq_s
open Words.Two_s
open Words.Four_s
open Types_s
open Arch.Types
open X64.Machine_s
open X64.Memory
open X64.Stack_i
open X64.Vale.State
open X64.Vale.Decls
open X64.Vale.QuickCode
open X64.Vale.QuickCodes
open X64.Vale.InsBasic
open X64.Vale.InsMem
#endverbatim

#verbatim
open Opaque_s
open X64.Vale
open X64.Vale.StateLemmas
open X64.Vale.InsLemmas
open X64.Taint_Semantics
open X64.Memory
open X64.Stack_i
module S = X64.Bytes_Semantics_s
module TS = X64.Taint_Semantics_s

friend X64.Vale.StateLemmas // TODO: this shouldn't be here
friend X64.Vale.Decls

//TODO: Find a better place for this
val lemma_valid_taint_match: (t:tainted_operand) -> (s:state) -> Lemma
  (requires valid_operand t s)
  (ensures TS.taint_match (t_op_to_op t) (get_taint t) s.memTaint (state_to_S s).TS.state)
  [SMTPat (TS.taint_match (t_op_to_op t) (get_taint t) s.memTaint (state_to_S s).TS.state)]

let lemma_valid_taint_match o s = match o with
  | TMem m t ->
    let addr = eval_maddr m s in
    let aux (b:buffer64) (i:int) : Lemma
      (requires valid_maddr addr s.mem s.memTaint b i t)
      (ensures TS.taint_match (t_op_to_op o) (get_taint o) s.memTaint (state_to_S s).TS.state) = 
      X64.Memory.lemma_valid_taint64 b s.memTaint s.mem i t
    in Classical.forall_intro_2 (fun b i -> (fun b -> Classical.move_requires (aux b)) b i)
  | _ -> ()

#endverbatim

#reset-options "--initial_fuel 2 --max_fuel 4 --max_ifuel 2 --z3rlimit 50"

var{:state xmm(0)} xmm0:quad32;
var{:state xmm(1)} xmm1:quad32;
var{:state xmm(2)} xmm2:quad32;
var{:state xmm(3)} xmm3:quad32;
var{:state xmm(4)} xmm4:quad32;
var{:state xmm(5)} xmm5:quad32;
var{:state xmm(6)} xmm6:quad32;
var{:state xmm(7)} xmm7:quad32;
var{:state xmm(8)} xmm8:quad32;
var{:state xmm(9)} xmm9:quad32;
var{:state xmm(10)} xmm10:quad32;
var{:state xmm(11)} xmm11:quad32;
var{:state xmm(12)} xmm12:quad32;
var{:state xmm(13)} xmm13:quad32;
var{:state xmm(14)} xmm14:quad32;
var{:state xmm(15)} xmm15:quad32;

operand_type xmm:quad32 @ nat4 :=
| inout xmm0 | inout xmm1 | inout xmm2 | inout xmm3
| inout xmm4 | inout xmm5 | inout xmm6 | inout xmm7
| inout xmm8 | inout xmm9 | inout xmm10 | inout xmm11
| inout xmm12 | inout xmm13 | inout xmm14 | inout xmm15
;
operand_type Mem128(base:reg64, inline offset:int, inline t:taint, ghost b:buffer128, ghost index:int):quad32;
operand_type opr128:quad32 := xmm | Mem128;

procedure{:operand} Mem128_in(base:reg64, inline offset:int, inline t:taint, ghost b:buffer128, ghost index:int) returns(o:opr)
    reads
        mem;
    extern;

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Paddd(dst, src)), Public))}{:quick exportOnly}{:public}
          Paddd(inout dst:xmm, src:xmm)
    modifies efl;
    ensures
        dst == old(add_wrap_quad32(dst, src));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPaddd(dst, src1, src2)), Public))}{:quick exportOnly}{:public}
          VPaddd(out dst:xmm, src1:xmm, src2:xmm)
    modifies efl;
    ensures
        dst == old(add_wrap_quad32(src1, src2));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pxor(dst, src)), Public))}{:quick exportOnly}{:public} Pxor(inout dst:xmm, src:xmm)
  ensures
    dst == old(quad32_xor(dst, src));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPxor(dst, src1, t_op_to_op128(src2))), Public))}{:quick exportOnly}{:public} VPxor(out dst:xmm, src1:xmm, src2:opr128)
  ensures
    dst == old(quad32_xor(src1, src2));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPxor(dst, src1, Mov128Mem(MReg(get_reg(src2), offset)))), t))}{:quick exportOnly}{:public} 
   VPxor_buffer(out dst:xmm, 
                src1:xmm, 
                src2:reg_opr64,
                inline offset:int,
                inline t:taint,
                ghost b:buffer128,
                ghost index:int)
  reads
    mem; memTaint;
  requires
    valid_src_addr(mem, b, index);
    valid_taint_buf128(b, mem, memTaint, t);
    src2 + offset == buffer_addr(b, mem) + 16 * index;
  ensures
    dst == old(quad32_xor(src1, buffer128_read(b, index, mem)));
    valid_taint_buf128(b, mem, memTaint, t);  // REVIEW: Why is this needed?
{
    low_lemma_valid_mem128(b, #nat(index), mem);
    low_lemma_load_mem128(b, #nat(index), mem);
    lemma_valid_taint128(b, #memTaint_t(memTaint), mem, #nat(index), t);
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pslld(dst, amt)), Public))}{:quick exportOnly}{:public} Pslld(inout dst:xmm, inline amt:int)
    requires
        0 <= amt < 32;
    ensures
        dst == four_map((lambda i:nat32 :: ishl32(i, amt)), old(dst));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Psrld(dst, amt)), Public))}{:quick exportOnly}{:public} Psrld(inout dst:xmm, inline amt:int)
    requires
        0 <= amt < 32;
    ensures
        dst == four_map((lambda i:nat32 :: ishr32(i, amt)), old(dst));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Psrldq(dst, amt)), Public))}{:quick exportOnly} Psrldq(inout dst:xmm, inline amt:int)
    requires
        0 <= amt < 16;
    ensures
        let src_bytes := le_quad32_to_bytes(old(dst));
        let zero_pad := Seq.create(#nat(amt), #nat8(0));
        let remaining_bytes := Seq.slice(src_bytes, #nat(amt), Seq.length(src_bytes));
        dst == le_bytes_to_quad32(Seq.append(zero_pad, remaining_bytes));
{
}


procedure{:instruction Ins(TS.TaintedIns(tuple(S.Palignr(dst, src, 4)), Public))}{:quick exportOnly}{:public}
          Palignr4(inout dst:xmm, src:xmm)
    modifies efl;
    ensures
        dst == old(Mkfour(src.lo1, src.hi2, src.hi3, dst.lo0));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Palignr(dst, src, 8)), Public))}{:quick exportOnly}{:public}
          Palignr8(inout dst:xmm, src:xmm)
    modifies efl;
    ensures
        dst == old(Mkfour(src.hi2, src.hi3, dst.lo0, dst.lo1));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPalignr(dst, src1, src2, 8)), Public))}{:quick exportOnly}{:public}
          VPalignr8(out dst:xmm, src1:xmm, src2:xmm)
    modifies efl;
    ensures
        dst == old(Mkfour(src2.hi2, src2.hi3, src1.lo0, src1.lo1));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Shufpd(dst, src, permutation)), Public))}{:quick exportOnly}{:public}
          Shufpd(inout dst:xmm, src:xmm, inline permutation:imm8)
    requires permutation < 4;
    modifies efl;
    ensures
        dst == old(Mkfour(
                    (if permutation = 0 || permutation = 2 then dst.lo0 else dst.hi2),
                    (if permutation = 0 || permutation = 2 then dst.lo1 else dst.hi3),
                    (if permutation = 0 || permutation = 1 then src.lo0 else src.hi2),
                    (if permutation = 0 || permutation = 1 then src.lo1 else src.hi3)));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VShufpd(dst, src1, src2, permutation)), Public))}{:quick exportOnly}{:public}
          VShufpd(out dst:xmm, src1:xmm, src2:xmm, inline permutation:imm8)
    requires permutation < 4;
    modifies efl;
    ensures
        dst == old(Mkfour(
                    (if permutation = 0 || permutation = 2 then src1.lo0 else src1.hi2),
                    (if permutation = 0 || permutation = 2 then src1.lo1 else src1.hi3),
                    (if permutation = 0 || permutation = 1 then src2.lo0 else src2.hi2),
                    (if permutation = 0 || permutation = 1 then src2.lo1 else src2.hi3)));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pshufb(dst, src)), Public))}{:quick exportOnly}{:public}
          Pshufb(inout dst:xmm, src:xmm)
    requires
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    modifies efl;
    ensures
        dst == reverse_bytes_quad32(old(dst));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPshufb(dst, src1, src2)), Public))}{:quick exportOnly}{:public}
          VPshufb(out dst:xmm, src1:xmm, src2:xmm)
    requires
        src2 == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    modifies efl;
    ensures
        dst == reverse_bytes_quad32(old(src1));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pshufb(dst, src)), Public))}{:quick exportOnly}{:public}
          PshufbDup(inout dst:xmm, src:xmm)
    requires
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x0C0D0E0F, 0x08090A0B);
    modifies efl;
    ensures
        dst == old(Mkfour(reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2),
                          reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2)));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pshufb(dst, src)), Public))}{:quick exportOnly}{:public}
          Pshufb64(inout dst:xmm, src:xmm)
    requires
        src == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
    modifies efl;
    ensures
        dst == old(Mkfour(reverse_bytes_nat32(dst.lo1),
                          reverse_bytes_nat32(dst.lo0),
                          reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2)));
{
}

/*
procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pshufb(dst, src)), Public))}{:quick exportOnly}
          Pshufb(inout dst:xmm, src:xmm)
    requires
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203) ||
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x0C0D0E0F, 0x08090A0B) ||
        src == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
    modifies efl;
    ensures
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203) ==>
            dst == reverse_bytes_quad32(old(dst));
        src == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x0C0D0E0F, 0x08090A0B) ==>
            dst == Mkfour(reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2),
                          reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2));
        src == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B) ==>
            dst == Mkfour(reverse_bytes_nat32(dst.lo1),
                          reverse_bytes_nat32(dst.lo0),
                          reverse_bytes_nat32(dst.hi3),
                          reverse_bytes_nat32(dst.hi2));
{
}
*/

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pshufd(dst, src, permutation)), Public))}{:quick exportOnly}{:public}
          Pshufd(inout dst:xmm, src:xmm, inline permutation:imm8)
    ensures
        dst == old(Mkfour(
            select_word(src, byte_to_twobits(permutation).lo0),
            select_word(src, byte_to_twobits(permutation).lo1),
            select_word(src, byte_to_twobits(permutation).hi2),
            select_word(src, byte_to_twobits(permutation).hi3)));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pcmpeqd(dst, src)), Public))}{:quick exportOnly}
          Pcmpeqd(inout dst:xmm, src:xmm)
    ensures
        dst == old(Mkfour(
            if src.lo0 = dst.lo0 then 0xFFFFFFFF else 0,
            if src.lo1 = dst.lo1 then 0xFFFFFFFF else 0,
            if src.hi2 = dst.hi2 then 0xFFFFFFFF else 0,
            if src.hi3 = dst.hi3 then 0xFFFFFFFF else 0));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pextrq(t_op_to_op(dst), src, index)), Public))}{:quick exportOnly}{:public}
          Pextrq(out dst:dst_opr64, src:xmm, inline index:imm8)
    requires
        index < 2;
    ensures
        dst == (if index = 0 then lo64(src) else hi64(src));
{
    reveal lo64_def;
    reveal hi64_def;
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pinsrd(dst, t_op_to_op(src), index)), get_taint(src)))}{:quick exportOnly}{:public}
          Pinsrd(inout dst:xmm, src:opr64, inline index:imm8)
    requires
        src < pow2_32;
        index < 4;
    ensures
        dst == insert_nat32(old(dst), #nat32(old(src)), #nat2(index));
{
}

procedure {:quick exportOnly}{:public}
          PinsrdImm(inout dst:xmm, inline immediate:nat32, inline index:imm8, out tmp:reg_opr64)
    requires
        index < 4;
    ensures
        dst == insert_nat32(old(dst), immediate, #nat2(index));
        tmp == immediate;
{
    Mov64(tmp, immediate);
    Pinsrd(dst, tmp, index);
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Pinsrq(dst, t_op_to_op(src), index)), get_taint(src)))}{:quick exportOnly}{:public}
          Pinsrq(inout dst:xmm, src:opr64, inline index:imm8)
    requires
        index < 2;
    ensures
        dst == insert_nat64_opaque(old(dst), src, #nat1(index));
{
    reveal insert_nat64;
    assert dst == insert_nat64(old(dst), src, #nat1(index));
}

procedure {:quick exportOnly}{:public}
          PinsrqImm(inout dst:xmm, inline immediate:nat64, inline index:imm8, out tmp:reg_opr64)
    requires
        index < 2;
    ensures
        dst == insert_nat64_opaque(old(dst), immediate, #nat1(index));
{
    Mov64(tmp, immediate);
    Pinsrq(dst, tmp, index);
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPSLLDQ(dst, src, 4)), Public))}{:quick exportOnly}{:public} VPSLLDQ4(out dst:xmm, src:xmm)
    ensures
        dst == old(Mkfour(0, src.lo0, src.lo1, src.hi2));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.VPSLLDQ(dst, src, 8)), Public))}{:quick exportOnly}{:public} Vpslldq8(out dst:xmm, src:xmm)
    ensures
        dst == old(Mkfour(0, 0, src.lo0, src.lo1));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.Vpsrldq(dst, src, 8)), Public))}{:quick exportOnly}{:public} Vpsrldq8(out dst:xmm, src:xmm)
    ensures
        dst == old(Mkfour(src.hi2, src.hi3, 0, 0));
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.MOVDQU(Mov128Xmm(dst),Mov128Xmm(src))), Public))}{:quick exportOnly}{:public} Mov128(inout dst:xmm, src:xmm)
    ensures
        dst == old(src);
{
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.MOVDQU(Mov128Xmm(dst), Mov128Mem(MReg(get_reg(src), offset)))), t))}{:quick exportOnly}{:public} Load128_buffer(
    out dst:xmm,
        src:reg_opr64,
        inline offset:int,
        inline t:taint,
        ghost b:buffer128,
        ghost index:int)
    reads
        mem; memTaint;
    modifies

    requires
        valid_src_addr(mem, b, index);
        valid_taint_buf128(b, mem, memTaint, t);
        src + offset == buffer_addr(b, mem) + 16 * index;
    ensures
        dst == buffer128_read(b, index, mem);
        valid_taint_buf128(b, mem, memTaint, t);  // REVIEW: Why is this needed?
{
    low_lemma_valid_mem128(b, #nat(index), mem);
    low_lemma_load_mem128(b, #nat(index), mem);
    lemma_valid_taint128(b, #memTaint_t(memTaint), mem, #nat(index), t);
}

procedure{:instruction mk_taint_ins2(S.MovBe64, dst, TMem(MReg(get_reg(src), offset), t))}{:quick exportOnly}{:public} LoadBe64_buffer128(
    out dst:reg_opr64,
        src:reg_opr64,
        inline offset:int,
        inline t:taint,
        inline upper:bool,
        ghost b:buffer128,
        ghost index:int)
    reads
        mem; memTaint;
    modifies

    requires
        valid_src_addr(mem, b, index);
        valid_taint_buf128(b, mem, memTaint, t);
        src + offset == buffer_addr(b, mem) + 16 * index + (if upper then 8 else 0);
    ensures
        dst == reverse_bytes_nat64(if upper then hi64(buffer128_read(b, index, mem)) else lo64(buffer128_read(b, index, mem)));
        valid_taint_buf128(b, mem, memTaint, t);
{
    low_lemma_valid_mem128_64(b, #nat(index), mem);
    low_lemma_load_mem128_lo64(b, #nat(index), mem);
    low_lemma_load_mem128_hi64(b, #nat(index), mem);
    lemma_valid_taint128(b, #memTaint_t(memTaint), mem, #nat(index), t);
}

procedure{:instruction Ins(TS.TaintedIns(tuple(S.MOVDQU(Mov128Mem(MReg(get_reg(dst), offset)), Mov128Xmm(src))), t))}{:quick exportOnly}{:public} Store128_buffer(
        dst:reg_opr64,
        src:xmm,
        inline offset:int,
        inline t:taint,
        ghost b:buffer128,
        ghost index:int)
    reads
        memTaint;
    modifies
        mem;
    requires
        valid_dst_addr(mem, b, index);
        valid_taint_buf128(b, mem, memTaint, t);
        dst + offset == buffer_addr(b, mem) + 16 * index;
    ensures
        modifies_mem(loc_buffer(b), old(mem), mem);
        valid_taint_buf128(b, mem, memTaint, t);
        mem == old(buffer128_write(b, index, src, mem));
{
    low_lemma_valid_mem128(b, #nat(index), old(mem));
    low_lemma_store_mem128(b, #nat(index), old(src), old(mem));
    lemma_valid_taint128(b, old(#memTaint_t(memTaint)), old(mem), #nat(index), t);
    same_memTaint128(b, old(mem), mem, old(#memTaint_t(memTaint)), #memTaint_t(memTaint));
}

procedure{:instruction mk_taint_ins2(S.Mov64, TMem(MReg(get_reg(dst), offset), t), src)}{:quick exportOnly}{:public} Store64_buffer128(
        dst:reg_opr64,
        src:reg_opr64,
        inline offset:int,
        inline t:taint,
        inline upper:bool,
        ghost b:buffer128,
        ghost index:int)
    reads
        memTaint;
    modifies
        mem;
    requires
        valid_dst_addr(mem, b, index);
        valid_taint_buf128(b, mem, memTaint, t);
        dst + offset == buffer_addr(b, mem) + 16 * index + (if upper then 8 else 0);
    ensures
        modifies_mem(loc_buffer(b), old(mem), mem);
        valid_taint_buf128(b, mem, memTaint, t);
        mem == old(buffer128_write(b, index, insert_nat64_opaque(old(buffer128_read(b, index, mem)), src, if upper then 1 else 0), mem));
{
    low_lemma_valid_mem128_64(b, #nat(index), old(mem));
    low_lemma_store_mem128_lo64(b, #nat(index), old(src), old(mem));
    low_lemma_store_mem128_hi64(b, #nat(index), old(src), old(mem));
    lemma_valid_taint128(b, old(#memTaint_t(memTaint)), old(mem), #nat(index), t);
    same_memTaint128(b, old(mem), mem, old(#memTaint_t(memTaint)), #memTaint_t(memTaint));
}

procedure {:quick exportOnly}{:public} ZeroXmm(inout dst:xmm)
    modifies efl;
    ensures
        dst == Mkfour(0, 0, 0, 0);
{
    Pxor(dst, dst);
    Arch.Types.lemma_quad32_xor();
}

procedure {:quick exportOnly}{:public} InitPshufbMask(inout dst:xmm, out tmp:reg_opr64)
    modifies efl;
    ensures
        dst == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
{
    assert two_to_nat32(Mktwo(0x0C0D0E0F, 0x08090A0B)) == 0x08090A0B0C0D0E0F; // OBSERVE
    assert two_to_nat32(Mktwo(0x04050607, 0x00010203)) == 0x0001020304050607; // OBSERVE
    PinsrqImm(dst, 0x08090A0B0C0D0E0F, 0, tmp);
    PinsrqImm(dst, 0x0001020304050607, 1, tmp);
    reveal insert_nat64;
}

procedure {:quick exportOnly}{:public} InitPshufbDupMask(inout dst:xmm, out tmp:reg_opr64)
    modifies efl;
    ensures
        dst == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x0C0D0E0F, 0x08090A0B);
{
    assert two_to_nat32(Mktwo(0x0C0D0E0F, 0x08090A0B)) == 0x08090A0B0C0D0E0F; // OBSERVE
    PinsrqImm(dst, 0x08090A0B0C0D0E0F, 0, tmp);
    PinsrqImm(dst, 0x08090A0B0C0D0E0F, 1, tmp);
    reveal insert_nat64;
}

procedure {:quick exportOnly}{:public} InitPshufb64Mask(inout dst:xmm, out tmp:reg_opr64)
    modifies efl;
    ensures
        dst == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
{
    assert two_to_nat32(Mktwo(0x0C0D0E0F, 0x08090A0B)) == 0x08090A0B0C0D0E0F; // OBSERVE
    assert two_to_nat32(Mktwo(0x04050607, 0x00010203)) == 0x0001020304050607; // OBSERVE
    PinsrqImm(dst, 0x0001020304050607, 0, tmp);
    PinsrqImm(dst, 0x08090A0B0C0D0E0F, 1, tmp);
    reveal insert_nat64;
}

// TODO: Lots of room to optimize this using bitwise operations
procedure {:quick}{:public} XmmEqual()
    lets x1 @= xmm0; x2 @= xmm1; tmp @= rdx; result @= rax;
    reads x2;
    modifies x1; tmp; result; efl;
    ensures
        if old(x1 = x2) then result = 0 else result > 0;
{
    Pcmpeqd(x1, x2);
    Pextrq(tmp, x1, 0);
    let tmp1 := tmp;
    lemma_equality_check_helper(x1);
    if (tmp == 0xFFFFFFFFFFFFFFFF) {
        //assert old(x1.lo0 == x2.lo0 /\ x1.lo1 == x2.lo1);
        Mov64(result, 0);
    } else {
        //assert old(x1.lo0 != x2.lo0) \/ (x1.lo1 != x2.lo1);
        Mov64(result, 1);
    }
    let result1 := result;
    Pextrq(tmp, x1, 1);
    let tmp2 := tmp;
    if (tmp == 0xFFFFFFFFFFFFFFFF) {
        //assert old(x1.hi2 == x2.hi2 /\ x1.hi3 == x2.hi3);
        Mov64(tmp, 0);
    } else {
        //assert old(x1.hi2 != x2.hi2) \/ (x1.hi3 != x2.hi3);
        Mov64(tmp, 1);
    }
    Add64(result, tmp);
//    assert tmp1 == lo64(x1);
//    assert tmp2 == hi64(x1);
//    assert result1 == (if tmp1 = 0xFFFFFFFFFFFFFFFF then 0 else 1);
//    assert tmp == (if tmp2 = 0xFFFFFFFFFFFFFFFF then 0 else 1);
//    assert result == tmp + result1;
    lemma_equality_check_helper_2(old(x1), old(x2), x1, tmp1, result1, tmp2, tmp, result);
}

/*  REVIEW: This version goes through with --ONE, but fails without it for reasons that are unclear
#reset-options "--z3rlimit 150 --z3refresh"
procedure {:quick exportOnly} XmmEqual(inout x1:xmm, x2:xmm, out tmp:reg_opr64, out result:reg_opr64)
    modifies efl;
    ensures
//        if old(x1 = x2) then result = 0 else result > 0;
{
    Pcmpeqd(x1, x2);
    Pextrq(tmp, x1, 0);
    ghost var tmp1 := tmp;
    lemma_equality_check_helper(x1);
    if (tmp == 0xFFFFFFFFFFFFFFFF) {
        assert old(x1.lo0 == x2.lo0 /\ x1.lo1 == x2.lo1);
        Mov64(result, 0);
    } else {
        assert old(not(x1.lo0 = x2.lo0) \/ not(x1.lo1 = x2.lo1));
        Mov64(result, 1);
    }
    ghost var result1 := result;
    Pextrq(tmp, x1, 1);
    ghost var tmp2 := tmp;
    if (tmp == 0xFFFFFFFFFFFFFFFF) {
        assert old(x1.hi2 == x2.hi2 /\ x1.hi3 == x2.hi3);
        Mov64(tmp, 0);
    } else {
        assert old(not(x1.hi2 = x2.hi2) \/ not(x1.hi3 = x2.hi3));
        Mov64(tmp, 1);
    }
    Add64(result, tmp);
    assert tmp1 == lo64(x1);
    assert tmp2 == hi64(x1);
    assert result1 == (if tmp1 = 0xFFFFFFFFFFFFFFFF then 0 else 1);
//    assert tmp == (if tmp2 = 0xFFFFFFFFFFFFFFFF then 0 else 1);
//    assert result == tmp + result1;
    lemma_equality_check_helper_2(old(x1), old(x2), x1, tmp1, result1, tmp2, tmp, result);
}
*/
