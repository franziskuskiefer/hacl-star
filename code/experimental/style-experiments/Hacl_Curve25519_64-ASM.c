/* This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
 * KreMLin invocation: /Users/bhargava/Desktop/repositories/kremlin//krml -funroll-loops 8 -warn-error +9 -I /Users/bhargava/Desktop/repositories/hacl-star//lib/ -I /Users/bhargava/Desktop/repositories/hacl-star//lib/fst -I /Users/bhargava/Desktop/repositories/kremlin//kremlib -I /Users/bhargava/Desktop/repositories/hacl-star//specs -I . -ccopt -march=native -fbuiltin-uint128 -drop FStar.UInt128 -fnocompound-literals -fc89-scope -fparentheses -fcurly-braces -tmpdir curve64-c curve64-c/out.krml -skip-compilation -minimal -add-include "kremlib.h" -bundle Hacl.Curve25519_64=*
 * F* version: 49558b40
 * KreMLin version: 557abd04
 */

#include "Hacl_Curve25519_64.h"

typedef struct K___uint64_t_uint64_t_s
{
  uint64_t fst;
  uint64_t snd;
}
K___uint64_t_uint64_t;

#include "x86intrin.h"

inline static K___uint64_t_uint64_t
Hacl_Impl_Curve25519_Field64_addcarry(uint64_t x, uint64_t y, uint64_t cin) {
  K___uint64_t_uint64_t res;
  res.snd = _addcarry_u64(cin,x,y,&res.fst);
  return res;
}

inline static K___uint64_t_uint64_t
Hacl_Impl_Curve25519_Field64_subborrow(uint64_t x, uint64_t y, uint64_t cin) {
  K___uint64_t_uint64_t res;
  res.snd = _subborrow_u64(cin,x,y,&res.fst);
  return res;
}


static void Hacl_Impl_Curve25519_Field64_fadd(uint64_t *c, uint64_t *a, uint64_t *b)
{
  __asm__ __volatile__(
    "mov     $38, %%eax ;"
    "xorl  %%ecx, %%ecx ;"
    "movq   (%2),  %%r8 ;"  "adcx   (%1),  %%r8 ;"
    "movq  8(%2),  %%r9 ;"  "adcx  8(%1),  %%r9 ;"
    "movq 16(%2), %%r10 ;"  "adcx 16(%1), %%r10 ;"
    "movq 24(%2), %%r11 ;"  "adcx 24(%1), %%r11 ;"
    "cmovc %%eax, %%ecx ;"
    "xorl %%eax, %%eax  ;"
    "adcx %%rcx,  %%r8  ;"
    "adcx %%rax,  %%r9  ;"  "movq  %%r9,  8(%0) ;"
    "adcx %%rax, %%r10  ;"  "movq %%r10, 16(%0) ;"
    "adcx %%rax, %%r11  ;"  "movq %%r11, 24(%0) ;"
    "mov     $38, %%ecx ;"
    "cmovc %%ecx, %%eax ;"
    "addq %%rax,  %%r8  ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rcx", "%r8", "%r9", "%r10", "%r11"
  );
}

  
inline static void Hacl_Impl_Curve25519_Field64_fsub(uint64_t *c, uint64_t *a, uint64_t *b){
  __asm__ __volatile__(
    "mov     $38, %%eax ;"
    "movq   (%1),  %%r8 ;"  "subq   (%2),  %%r8 ;"
    "movq  8(%1),  %%r9 ;"  "sbbq  8(%2),  %%r9 ;"
    "movq 16(%1), %%r10 ;"  "sbbq 16(%2), %%r10 ;"
    "movq 24(%1), %%r11 ;"  "sbbq 24(%2), %%r11 ;"
    "mov      $0, %%ecx ;"
    "cmovc %%eax, %%ecx ;"
    "subq %%rcx,  %%r8  ;"
    "sbbq    $0,  %%r9  ;"  "movq  %%r9,  8(%0) ;"
    "sbbq    $0, %%r10  ;"  "movq %%r10, 16(%0) ;"
    "sbbq    $0, %%r11  ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx  ;"
    "cmovc %%eax, %%ecx ;"
    "subq %%rcx,  %%r8  ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rcx", "%r8", "%r9", "%r10", "%r11"
  );
}


inline static void
Hacl_Impl_Curve25519_Field64_mul_felem(uint64_t *c, uint64_t *a, uint64_t *b) {
  __asm__ __volatile__(
    "movq   (%1), %%rdx; " /* A[0] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[0]*B[0] */    "xorl %%r10d, %%r10d ;"                           "movq  %%r8,  (%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[0]*B[1] */    "adox  %%r9, %%r10 ;"                             "movq %%r10, 8(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[0]*B[2] */    "adox %%r11, %%r12 ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[0]*B[3] */    "adox %%r13, %%r14 ;"                                                       "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"

    "movq  8(%1), %%rdx; " /* A[1] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[1]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 8(%0),  %%r8 ;"    "movq  %%r8,  8(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[1]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 16(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[1]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"                              "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[1]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"                              "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"

    "movq 16(%1), %%rdx; " /* A[2] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[2]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 16(%0), %%r8 ;"    "movq  %%r8, 16(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[2]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 24(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[2]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"                              "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[2]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"                              "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"

    "movq 24(%1), %%rdx; " /* A[3] */
    "mulx   (%2),  %%r8,  %%r9; " /* A[3]*B[0] */    "xorl %%r10d, %%r10d ;"  "adcx 24(%0), %%r8 ;"    "movq  %%r8, 24(%0) ;"
    "mulx  8(%2), %%r10, %%r11; " /* A[3]*B[1] */    "adox  %%r9, %%r10 ;"    "adcx %%r12, %%r10 ;"    "movq %%r10, 32(%0) ;"
    "mulx 16(%2), %%r12, %%r13; " /* A[3]*B[2] */    "adox %%r11, %%r12 ;"    "adcx %%r14, %%r12 ;"    "movq %%r12, 40(%0) ;"    "movq $0, %%r8  ;"
    "mulx 24(%2), %%r14, %%rdx; " /* A[3]*B[3] */    "adox %%r13, %%r14 ;"    "adcx %%rax, %%r14 ;"    "movq %%r14, 48(%0) ;"    "movq $0, %%rax ;"
    /*******************************************/    "adox %%rdx, %%rax ;"    "adcx  %%r8, %%rax ;"    "movq %%rax, 56(%0) ;"
  :
  : "r" (c), "r" (a), "r" (b)
  : "memory", "cc", "%rax", "%rdx", "%r8",
    "%r9", "%r10", "%r11", "%r12", "%r13", "%r14"
  );
}

inline static void Hacl_Impl_Curve25519_Field64_carry_wide(uint64_t *c, uint64_t *a){
  __asm__ __volatile__(
    "movl    $38, %%edx ;" /* 2*c = 38 = 2^256 */
    "mulx 32(%1),  %%r8, %%r10 ;" /* c*C[4] */  "xorl %%ebx, %%ebx ;"  "adox   (%1),  %%r8 ;"
    "mulx 40(%1),  %%r9, %%r11 ;" /* c*C[5] */  "adcx %%r10,  %%r9 ;"  "adox  8(%1),  %%r9 ;"
    "mulx 48(%1), %%r10, %%rax ;" /* c*C[6] */  "adcx %%r11, %%r10 ;"  "adox 16(%1), %%r10 ;"
    "mulx 56(%1), %%r11, %%rcx ;" /* c*C[7] */  "adcx %%rax, %%r11 ;"  "adox 24(%1), %%r11 ;"
    /****************************************/  "adcx %%rbx, %%rcx ;"  "adox  %%rbx, %%rcx ;"
    "imul %%rdx, %%rcx ;" /* c*C[4], cf=0, of=0 */
    "adcx %%rcx,  %%r8 ;"
    "adcx %%rbx,  %%r9 ;"  "movq  %%r9,  8(%0) ;"
    "adcx %%rbx, %%r10 ;"  "movq %%r10, 16(%0) ;"
    "adcx %%rbx, %%r11 ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rbx", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11"
  );
}


inline static void Hacl_Impl_Curve25519_Field64_fmul(uint64_t *out, uint64_t *f1, uint64_t *f2)
{
  uint64_t tmp_w[8U] = { 0U };
  Hacl_Impl_Curve25519_Field64_mul_felem(tmp_w, f1, f2);
  Hacl_Impl_Curve25519_Field64_carry_wide(out, tmp_w);
}

inline static void Hacl_Impl_Curve25519_Field64_fmul2(uint64_t *out1, uint64_t* out2, int64_t *f1, uint64_t *f2, uint64_t *f3, uint64_t *f4)
{
  uint64_t tmp_w[16U] = { 0U };
  uint64_t* tmp_w1 = tmp_w;
  uint64_t* tmp_w2 = tmp_w+8;
  Hacl_Impl_Curve25519_Field64_mul_felem(tmp_w1, f1, f2);
  Hacl_Impl_Curve25519_Field64_mul_felem(tmp_w2, f3, f4);
  Hacl_Impl_Curve25519_Field64_carry_wide(out1, tmp_w1);
  Hacl_Impl_Curve25519_Field64_carry_wide(out2, tmp_w2);
}

inline static void Hacl_Impl_Curve25519_Field64_fmul1(uint64_t *c, uint64_t *a){
  const uint64_t a24 = 121665;
  __asm__ __volatile__(
    "movq     %2, %%rdx ;"
    "mulx   (%1),  %%r8, %%r10 ;"
    "mulx  8(%1),  %%r9, %%r11 ;"  "addq %%r10,  %%r9 ;"
    "mulx 16(%1), %%r10, %%rax ;"  "adcq %%r11, %%r10 ;"
    "mulx 24(%1), %%r11, %%rcx ;"  "adcq %%rax, %%r11 ;"
    /***************************/  "adcq    $0, %%rcx ;"
    "movl   $38, %%edx ;" /* 2*c = 38 = 2^256 mod 2^255-19*/
    "imul %%rdx, %%rcx ;"
    "addq %%rcx,  %%r8 ;"
    "adcq    $0,  %%r9 ;"  "movq  %%r9,  8(%0) ;"
    "adcq    $0, %%r10 ;"  "movq %%r10, 16(%0) ;"
    "adcq    $0, %%r11 ;"  "movq %%r11, 24(%0) ;"
    "mov     $0, %%ecx ;"
    "cmovc %%edx, %%ecx ;"
    "addq %%rcx,  %%r8 ;"  "movq  %%r8,   (%0) ;"
  :
  : "r" (c), "r" (a), "r" (a24)
  : "memory", "cc", "%rax", "%rcx", "%rdx", "%r8", "%r9", "%r10", "%r11"
  );
}

inline static void Hacl_Impl_Curve25519_Field64_fsqr_(uint64_t *c, uint64_t *a) {
  __asm__ __volatile__(
    "movq   (%1), %%rdx        ;" /* A[0]      */
    "mulx  8(%1),  %%r8, %%r14 ;" /* A[1]*A[0] */  "xorl %%r15d, %%r15d;"
    "mulx 16(%1),  %%r9, %%r10 ;" /* A[2]*A[0] */  "adcx %%r14,  %%r9 ;"
    "mulx 24(%1), %%rax, %%rcx ;" /* A[3]*A[0] */  "adcx %%rax, %%r10 ;"
    "movq 24(%1), %%rdx        ;" /* A[3]      */
    "mulx  8(%1), %%r11, %%r12 ;" /* A[1]*A[3] */  "adcx %%rcx, %%r11 ;"
    "mulx 16(%1), %%rax, %%r13 ;" /* A[2]*A[3] */  "adcx %%rax, %%r12 ;"
    "movq  8(%1), %%rdx        ;" /* A[1]      */  "adcx %%r15, %%r13 ;"
    "mulx 16(%1), %%rax, %%rcx ;" /* A[2]*A[1] */  "movq    $0, %%r14 ;"
    /*******************************************/  "adcx %%r15, %%r14 ;"

    "xorl %%r15d, %%r15d;"
    "adox %%rax, %%r10 ;"  "adcx  %%r8,  %%r8 ;"
    "adox %%rcx, %%r11 ;"  "adcx  %%r9,  %%r9 ;"
    "adox %%r15, %%r12 ;"  "adcx %%r10, %%r10 ;"
    "adox %%r15, %%r13 ;"  "adcx %%r11, %%r11 ;"
    "adox %%r15, %%r14 ;"  "adcx %%r12, %%r12 ;"
                           "adcx %%r13, %%r13 ;"
                           "adcx %%r14, %%r14 ;"

    "movq   (%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[0]^2 */
    /********************/  "movq %%rax,  0(%0) ;"
    "addq %%rcx,  %%r8 ;"   "movq  %%r8,  8(%0) ;"
    "movq  8(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[1]^2 */
    "adcq %%rax,  %%r9 ;"   "movq  %%r9, 16(%0) ;"
    "adcq %%rcx, %%r10 ;"   "movq %%r10, 24(%0) ;"
    "movq 16(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[2]^2 */
    "adcq %%rax, %%r11 ;"   "movq %%r11, 32(%0) ;"
    "adcq %%rcx, %%r12 ;"   "movq %%r12, 40(%0) ;"
    "movq 24(%1), %%rdx ;"  "mulx %%rdx, %%rax, %%rcx ;" /* A[3]^2 */
    "adcq %%rax, %%r13 ;"   "movq %%r13, 48(%0) ;"
    "adcq %%rcx, %%r14 ;"   "movq %%r14, 56(%0) ;"
  :
  : "r" (c), "r" (a)
  : "memory", "cc", "%rax", "%rcx", "%rdx",
    "%r8", "%r9", "%r10", "%r11", "%r12", "%r13", "%r14", "%r15"
  );
}

inline static void Hacl_Impl_Curve25519_Field64_fsqr(uint64_t *out, uint64_t *f)
{
//  Hacl_Impl_Curve25519_Field64_fmul(out, f, f);
 uint64_t tmp_w[8U] = { 0U };
 Hacl_Impl_Curve25519_Field64_fsqr_(tmp_w, f);
 Hacl_Impl_Curve25519_Field64_carry_wide(out, tmp_w);
}


inline static void Hacl_Impl_Curve25519_Field64_fsqr2(uint64_t *out1, uint64_t* out2, uint64_t *f1, uint64_t* f2)
{
//  Hacl_Impl_Curve25519_Field64_fmul(out, f, f);
 uint64_t tmp_w[16U] = { 0U };
 uint64_t* tmp_w1 = tmp_w;
 uint64_t* tmp_w2 = tmp_w + 8;
 Hacl_Impl_Curve25519_Field64_fsqr_(tmp_w1, f1);
 Hacl_Impl_Curve25519_Field64_fsqr_(tmp_w2, f2);
 Hacl_Impl_Curve25519_Field64_carry_wide(out1, tmp_w1);
 Hacl_Impl_Curve25519_Field64_carry_wide(out2, tmp_w2);
}


static void Hacl_Impl_Curve25519_Field64_store_felem(uint64_t *u64s, uint64_t *f)
{
  uint64_t f0 = f[0U];
  uint64_t f1 = f[1U];
  uint64_t f2 = f[2U];
  uint64_t f3 = f[3U];
  uint64_t top_bit = f3 >> (uint32_t)63U;
  uint64_t f31 = f3 & (uint64_t)0x7fffffffffffffffU;
  K___uint64_t_uint64_t
  scrut0 = Hacl_Impl_Curve25519_Field64_addcarry(f0, (uint64_t)19U * top_bit, (uint64_t)0U);
  uint64_t f01 = scrut0.fst;
  uint64_t carry1 = scrut0.snd;
  K___uint64_t_uint64_t scrut1 = Hacl_Impl_Curve25519_Field64_addcarry(f1, (uint64_t)0U, carry1);
  uint64_t f11 = scrut1.fst;
  uint64_t carry2 = scrut1.snd;
  K___uint64_t_uint64_t scrut2 = Hacl_Impl_Curve25519_Field64_addcarry(f2, (uint64_t)0U, carry2);
  uint64_t f21 = scrut2.fst;
  uint64_t carry3 = scrut2.snd;
  K___uint64_t_uint64_t
  scrut3 = Hacl_Impl_Curve25519_Field64_addcarry(f31, (uint64_t)0U, carry3);
  uint64_t f32 = scrut3.fst;
  uint64_t top_bit1 = f32 >> (uint32_t)63U;
  uint64_t f33 = f32 & (uint64_t)0x7fffffffffffffffU;
  K___uint64_t_uint64_t
  scrut = Hacl_Impl_Curve25519_Field64_addcarry(f01, (uint64_t)19U * top_bit1, (uint64_t)0U);
  uint64_t f02 = scrut.fst;
  uint64_t carry5 = scrut.snd;
  K___uint64_t_uint64_t
  scrut4 = Hacl_Impl_Curve25519_Field64_addcarry(f11, (uint64_t)0U, carry5);
  uint64_t f12 = scrut4.fst;
  uint64_t carry6 = scrut4.snd;
  K___uint64_t_uint64_t
  scrut5 = Hacl_Impl_Curve25519_Field64_addcarry(f21, (uint64_t)0U, carry6);
  uint64_t f22 = scrut5.fst;
  uint64_t carry7 = scrut5.snd;
  uint64_t f34 = f33 + carry7;
  u64s[0U] = f02;
  u64s[1U] = f12;
  u64s[2U] = f22;
  u64s[3U] = f34;
}

inline static void
Hacl_Impl_Curve25519_Generic_fsquare_times_64(uint64_t *o, uint64_t *i, uint32_t n1)
{
  uint32_t i0;
  Hacl_Impl_Curve25519_Field64_fsqr(o, i);
  for (i0 = (uint32_t)0U; i0 < n1 - (uint32_t)1U; i0 = i0 + (uint32_t)1U)
  {
    Hacl_Impl_Curve25519_Field64_fsqr(o, o);
  }
}

static void Hacl_Impl_Curve25519_Generic_finv_64(uint64_t *o, uint64_t *i)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t tmp[(uint32_t)4U * (uint32_t)4U];
    memset(tmp, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp[0U]);
    {
      void **a = (void **)tmp;
      void **b = (void **)(tmp + (uint32_t)4U);
      void **c = (void **)(tmp + (uint32_t)2U * (uint32_t)4U);
      void **t0 = (void **)(tmp + (uint32_t)3U * (uint32_t)4U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)a, i, (uint32_t)1U);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)a, (uint32_t)2U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)b, (uint64_t *)t0, i);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)a, (uint64_t *)b, (uint64_t *)a);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)a, (uint32_t)1U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)5U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)10U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)c, (uint32_t)20U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)10U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)b, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)b, (uint32_t)50U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)c, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)c, (uint32_t)100U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)c);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)50U);
      Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)t0, (uint64_t *)t0, (uint64_t *)b);
      Hacl_Impl_Curve25519_Generic_fsquare_times_64((uint64_t *)t0, (uint64_t *)t0, (uint32_t)5U);
      Hacl_Impl_Curve25519_Field64_fmul(o, (uint64_t *)t0, (uint64_t *)a);
    }
  }
}

static void Hacl_Impl_Curve25519_Generic_decode_point_64(uint64_t *o, uint8_t *i)
{
  uint64_t tmp[4U] = { 0U };
  void **x;
  void **z;
  {
    uint8_t *b_i = i + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[0U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[1U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[2U] = u_i;
  }
  {
    uint8_t *b_i = i + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    tmp[3U] = u_i;
  }
  tmp[3U] = tmp[3U] & (uint64_t)0x7fffffffffffffffU;
  x = (void **)o;
  z = (void **)(o + (uint32_t)4U);
  ((uint64_t *)z)[0U] = (uint64_t)0U;
  ((uint64_t *)z)[1U] = (uint64_t)0U;
  ((uint64_t *)z)[2U] = (uint64_t)0U;
  ((uint64_t *)z)[3U] = (uint64_t)0U;
  ((uint64_t *)z)[0U] = ((uint64_t *)z)[0U] | (uint64_t)1U;
  ((uint64_t *)x)[0U] = tmp[0U];
  ((uint64_t *)x)[1U] = tmp[1U];
  ((uint64_t *)x)[2U] = tmp[2U];
  ((uint64_t *)x)[3U] = tmp[3U];
}

static void Hacl_Impl_Curve25519_Generic_encode_point_64(uint8_t *o, uint64_t *i)
{
  void **x = (void **)i;
  void **z = (void **)(i + (uint32_t)4U);
  uint64_t buf[4U] = { 0U };
  void **tmp = (void **)buf;
  uint64_t u64s[4U] = { 0U };
  Hacl_Impl_Curve25519_Generic_finv_64((uint64_t *)tmp, (uint64_t *)z);
  Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)tmp, (uint64_t *)tmp, (uint64_t *)x);
  Hacl_Impl_Curve25519_Field64_store_felem(u64s, (uint64_t *)tmp);
  {
    uint64_t u_i = u64s[0U];
    uint8_t *b_i = o + (uint32_t)0U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[1U];
    uint8_t *b_i = o + (uint32_t)1U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[2U];
    uint8_t *b_i = o + (uint32_t)2U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
  {
    uint64_t u_i = u64s[3U];
    uint8_t *b_i = o + (uint32_t)3U * (uint32_t)8U;
    store64_le(b_i, u_i);
  }
}

static void
Hacl_Impl_Curve25519_Generic_point_add_and_double_64(
  uint64_t *q,
  uint64_t *nq,
  uint64_t *nq_p1
)
{
  uint64_t *x1 = q;
  uint64_t *x2 = nq;
  uint64_t *z2 = nq + (uint32_t)4U;
  uint64_t *x3 = nq_p1;
  uint64_t *z3 = nq_p1 + (uint32_t)4U;
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)4U * (uint32_t)4U);
  {
    uint64_t tmp[(uint32_t)4U * (uint32_t)4U];
    memset(tmp, 0U, (uint32_t)4U * (uint32_t)4U * sizeof tmp[0U]);
    {
      void **a = (void **)tmp;
      void **b = (void **)(tmp + (uint32_t)4U);
      void **c = (void **)(tmp + (uint32_t)2U * (uint32_t)4U);
      void **d = (void **)(tmp + (uint32_t)3U * (uint32_t)4U);
      Hacl_Impl_Curve25519_Field64_fadd((uint64_t *)a, x2, z2);
      Hacl_Impl_Curve25519_Field64_fsub((uint64_t *)b, x2, z2);
      Hacl_Impl_Curve25519_Field64_fadd((uint64_t *)c, x3, z3);
      Hacl_Impl_Curve25519_Field64_fsub((uint64_t *)d, x3, z3);
//    Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)d, (uint64_t *)d, (uint64_t *)a);
//    Hacl_Impl_Curve25519_Field64_fmul((uint64_t *)c, (uint64_t *)c, (uint64_t *)b);
      Hacl_Impl_Curve25519_Field64_fmul2((uint64_t *)c, (uint64_t *)d, (uint64_t *)c, (uint64_t *)b, (uint64_t *)d, (uint64_t *)a);

      Hacl_Impl_Curve25519_Field64_fadd(x3, (uint64_t *)d, (uint64_t *)c);
      Hacl_Impl_Curve25519_Field64_fsub(z3, (uint64_t *)d, (uint64_t *)c);
 //     Hacl_Impl_Curve25519_Field64_fsqr(x3, x3);
 //     Hacl_Impl_Curve25519_Field64_fsqr(z3, z3);
     Hacl_Impl_Curve25519_Field64_fsqr2(x3, z3, x3, z3);
      Hacl_Impl_Curve25519_Field64_fmul(z3, z3, x1);
//      Hacl_Impl_Curve25519_Field64_fsqr((uint64_t *)c, (uint64_t *)a);
//      Hacl_Impl_Curve25519_Field64_fsqr((uint64_t *)d, (uint64_t *)b);
      Hacl_Impl_Curve25519_Field64_fsqr2((uint64_t*)c,(uint64_t *)d, (uint64_t*)a,(uint64_t *)b);
      Hacl_Impl_Curve25519_Field64_fmul(x2, (uint64_t *)c, (uint64_t *)d);
      Hacl_Impl_Curve25519_Field64_fsub((uint64_t *)b, (uint64_t *)c, (uint64_t *)d);
      Hacl_Impl_Curve25519_Field64_fmul1((uint64_t *)a, (uint64_t *)b);
      Hacl_Impl_Curve25519_Field64_fadd((uint64_t *)a, (uint64_t *)a, (uint64_t *)c);
      Hacl_Impl_Curve25519_Field64_fmul(z2, (uint64_t *)b, (uint64_t *)a);
    }
  }
}

inline static void
Hacl_Impl_Curve25519_Generic_cswap_64(uint64_t bit, uint64_t *p0, uint64_t *p1)
{
  uint64_t mask = (uint64_t)0U - bit;
  uint32_t i;
  for (i = (uint32_t)0U; i < (uint32_t)2U * (uint32_t)4U; i = i + (uint32_t)1U)
  {
    uint64_t dummy = mask & (p0[i] ^ p1[i]);
    p0[i] = p0[i] ^ dummy;
    p1[i] = p1[i] ^ dummy;
  }
}

static void
Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(
  uint64_t *out,
  uint64_t *key,
  uint64_t *init1
)
{
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
  {
    uint64_t p0[(uint32_t)2U * (uint32_t)4U];
    memset(p0, 0U, (uint32_t)2U * (uint32_t)4U * sizeof p0[0U]);
    KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
    {
      uint64_t p1[(uint32_t)2U * (uint32_t)4U];
      memset(p1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof p1[0U]);
      {
        void **x0;
        memcpy(p1, init1, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
        x0 = (void **)p0;
        ((uint64_t *)x0)[0U] = ((uint64_t *)x0)[0U] | (uint64_t)1U;
        {
          uint32_t i;
          for (i = (uint32_t)0U; i < (uint32_t)256U; i = i + (uint32_t)1U)
          {
            uint64_t
            bit =
              key[((uint32_t)255U - i)
              / (uint32_t)64U]
              >> ((uint32_t)255U - i) % (uint32_t)64U
              & (uint64_t)1U;
            Hacl_Impl_Curve25519_Generic_cswap_64(bit, p0, p1);
            Hacl_Impl_Curve25519_Generic_point_add_and_double_64(init1, p0, p1);
            Hacl_Impl_Curve25519_Generic_cswap_64(bit, p0, p1);
          }
        }
        memcpy(out, p0, (uint32_t)2U * (uint32_t)4U * sizeof p0[0U]);
      }
    }
  }
}

static uint8_t
Hacl_Impl_Curve25519_Generic_g25519[32U] =
  {
    (uint8_t)9U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U,
    (uint8_t)0U, (uint8_t)0U, (uint8_t)0U, (uint8_t)0U
  };

void Hacl_Curve25519_64_secret_to_public(uint8_t *pub, uint8_t *priv)
{
  uint8_t basepoint[32U] = { 0U };
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)32U; i = i + (uint32_t)1U)
    {
      basepoint[i] = Hacl_Impl_Curve25519_Generic_g25519[i];
    }
  }
  {
    uint64_t scalar[4U] = { 0U };
    {
      uint8_t *b_i = priv + (uint32_t)0U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[0U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)1U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[1U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)2U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[2U] = u_i;
    }
    {
      uint8_t *b_i = priv + (uint32_t)3U * (uint32_t)8U;
      uint64_t u = load64_le(b_i);
      uint64_t u_i = u;
      scalar[3U] = u_i;
    }
    scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
    scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
    scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
    KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
    {
      uint64_t init1[(uint32_t)2U * (uint32_t)4U];
      memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
      Hacl_Impl_Curve25519_Generic_decode_point_64(init1, basepoint);
      Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
      Hacl_Impl_Curve25519_Generic_encode_point_64(pub, init1);
    }
  }
}

void Hacl_Curve25519_64_ecdh(uint8_t *shared, uint8_t *my_priv, uint8_t *their_pub)
{
  uint64_t scalar[4U] = { 0U };
  {
    uint8_t *b_i = my_priv + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[0U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[1U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[2U] = u_i;
  }
  {
    uint8_t *b_i = my_priv + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(b_i);
    uint64_t u_i = u;
    scalar[3U] = u_i;
  }
  scalar[0U] = scalar[0U] & (uint64_t)0xfffffffffffffff8U;
  scalar[3U] = scalar[3U] & (uint64_t)0x7fffffffffffffffU;
  scalar[3U] = scalar[3U] | (uint64_t)0x4000000000000000U;
  KRML_CHECK_SIZE(sizeof (uint64_t), (uint32_t)2U * (uint32_t)4U);
  {
    uint64_t init1[(uint32_t)2U * (uint32_t)4U];
    memset(init1, 0U, (uint32_t)2U * (uint32_t)4U * sizeof init1[0U]);
    Hacl_Impl_Curve25519_Generic_decode_point_64(init1, their_pub);
    Hacl_Impl_Curve25519_Generic_montgomery_ladder_64(init1, scalar, init1);
    Hacl_Impl_Curve25519_Generic_encode_point_64(shared, init1);
  }
}

